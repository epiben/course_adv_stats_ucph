---
title: "Code for slide deck on cross-validation and penalised regression"
author: "Benjamin Skov Kaas-Hansen"
date: "18 Nov 2021"
output: 
  html_document: 
    df_print: paged
    highlight: haddock
    theme: paper
    toc: yes
    toc_depth: 2
  pdf_document: 
    highlight: haddock
    toc: yes
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

# Note

-   I've used quite some functions from `dplyr` (e.g., `%>%`, `mutate` and `select`) and `tidyr` (`gather`). If you don't understand what's going on in a "chain" (operations linked by `%>%`), try to run the chain sequentially (first only line 1, then lines 1 and 2, then lines 1-3, etc.) and see what happens.

# Setup

```{r, warning=FALSE, message=FALSE}
packages <- c("plyr", "tidyr", "broom", "modelr", "glmnet", "selectiveInference", "MASS", "tidyverse", "boot")
for (p in packages)
	library(p, character.only = TRUE)

knitr::opts_chunk$set(fig.align = "center")

theme_set(theme_minimal()+
		  	theme(axis.title = element_text(size = 11),
		  		  axis.text = element_text(size = 10),
		  		  strip.text = element_text(size = 11)))

# Little helper to get the glmnet coefficients in nice tidy format
pretty_coefs <- function(coefs) { # coefs: the output from coef(fit_object, s = [value])
	enframe(coefs[, 1], "predictor", "coefficient") %>% 
		filter(coefficient != 0) %>% 
		arrange(desc(abs(coefficient)))
}
```

# Generalised linear models

```{r}
data(fev, package = "mplot")
head(fev)
ggplot(fev, aes(x = age, y = fev)) +
	geom_jitter(height = 0, width = 0.25, size = 0.5, alpha = 0.25) 
	geom_smooth(se = FALSE) 
	geom_smooth(se = FALSE, method = "lm", colour = "black")

linmod <- lm(fev ~ age + height + I(height^2) + sex + smoke, data = fev)
tidy(linmod)

titanic <- as.data.frame(Titanic) %>% 
	uncount(Freq)
logreg <- glm(Survived ~ Class + Sex + Age, data = titanic, family = binomial(link = "logit"))
tidy(logreg)
```

# Cross-validation Pima

```{r}
data(PimaIndiansDiabetes2, package = "mlbench")
glimpse(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
```

## Homegrown

Very simplistic implementation but it illustrates the logic.

```{r}
set.seed(42)
pima <- na.exclude(PimaIndiansDiabetes2) %>% 
	mutate(cv_fold = sample(1:n() %% 10 + 1, replace = FALSE))
table(pima$cv_fold) # fairly equal distribution

err_cv <- c()
for (i in unique(pima$cv_fold)) {
	train <- filter(pima, cv_fold != i)
	mod <- glm(diabetes ~ age + mass + insulin + pregnant, data = train, family = binomial)
	
	val <- filter(pima, cv_fold == i)
	y_pred <- predict(mod, newdata = val, type = "response")
	y_true <- as.numeric(val$diabetes) - 1 # bring binary factor to 0/1 scale
	err <- mean(abs(y_true - y_pred) > 0.5)
	err_cv <- c(err_cv, err)
}
err_cv
mean(err_cv)
```

## Using packages

The approach you generally want to do (no need to re-invent the wheel).

```{r}
binary_pred_cost <- function(y_true, y_pred) {
	mean(abs(y_true - y_pred) > 0.5)
}

# Fit normal model
pima_glm <- glm(diabetes ~ age + mass + insulin + pregnant, data = pima, family = binomial)

# Use model object for cross-validation
pima_loo <- cv.glm(pima, pima_glm, cost = binary_pred_cost)
pima_loo$delta # 1st is raw estimate, 2nd is bias-corrected

pima_cv1 <- cv.glm(pima, pima_glm, cost = binary_pred_cost, K = 10)
pima_cv1$delta # 1st is raw estimate, 2nd is bias-corrected
```

The `modelr`-package has some powerful functionalities for CV, LOOCV and bootstrapping. It's more involved but may be worthwhile in real life.

# Lasso regression example: biopsies from breast cancer patients

## Lasso regression

Look at the data (in Danish: vi skal tegne, før vi må regne)

```{r}
data(biopsy)
summary(biopsy) # NA's in V6; mean varies across variables but anyway somewhere around 2 and 4

biopsy_complete <- na.exclude(biopsy) # remove rows with any missing value
biopsy_predictors <- biopsy_complete %>% 
	select(-ID, -class) %>%
	scale() # note attributes "remember" normlisation factors; useful for transforming test set

bind_rows(gather(as_tibble(biopsy_predictors), var, value) %>% 
		  	mutate(scale = "normalised"),
		  gather(select(biopsy_complete, -ID, -class), var, value) %>% 
		  	mutate(scale = "original")) %>% 
	ggplot(aes(x = value, colour = scale)) +
		geom_density(position = "identity") +
		scale_x_continuous(breaks = -2:10) +
		facet_wrap(~ var, scales = "free_y") +
		theme(axis.text.y = element_blank())
```

Fit model

```{r}
lasso_logreg <- glmnet(biopsy_predictors, biopsy_complete$class, family = "binomial")
```

Coefficient profile plot (built-in: ugly but easy). Below I've made a custom fit function using ggplot2 in case anyone's interested

```{r}
plot(lasso_logreg, xvar = "lambda", label = TRUE, lwd = 1.5)
```

And let's take a look at the non-zero coefficients (so the ones selected by the lasso regression). I've a custom function (`pretty_coefs`, see top of document) that returns these coefficients in a nice format. In general, if you find yourself doing the same (or almost the same) thing more than once, it's usually a good idea to pack that *thing* into a function. This gives shorter code, which is easier to read, maintain and debug.

```{r}
pretty_coefs(coef(lasso_logreg, s = exp(-2.5)))
```

Alternative way to find the non-zero coefficients (this is basically what I've packed into the `pretty_coefs` function.)

```{r}
lasso_logreg_coefs <- coef(lasso_logreg, s = exp(-1.5))
lasso_logreg_coefs[which(lasso_logreg_coefs != 0), 1]
```

## Ridge and elastic net models

```{r}
# ridge_logreg <- update(lasso_logreg, alpha = 0)
ridge_logreg <- glmnet(biopsy_predictors, biopsy_complete$class, family = "binomial", 
					   alpha = 0)
plot(ridge_logreg, xvar = "lambda")
pretty_coefs(coef(ridge_logreg, s = exp(2))) # all shrunk but no real selection

elastic_logreg <- update(lasso_logreg, alpha = 0.5)
plot(elastic_logreg, xvar = "lambda")
pretty_coefs(coef(elastic_logreg, s = exp(-1.2))) # all shrunk AND "ranking" (different coefficient sizes)

ldply(list(lasso = lasso_logreg, ridge = ridge_logreg, elastic = elastic_logreg),
	  function(.) data.frame(as.matrix(t(.$beta)), x = apply(abs(.$beta), 2, sum)), 
	  .id = "mod") %>% 
	gather(predictor, value, -x, -mod) %>% 
	mutate(mod = factor(mod, levels = c("lasso", "elastic", "ridge"),
						labels = c("Lasso", "Elastic net (alpha = 0.5)", "Ridge"))) %>% 
	ggplot(aes(x, value, colour = predictor)) +
		geom_line() +
		labs(x = "l1 norm of coefficients", y = "Coefficient value") +
		facet_wrap(~ mod, scales = "free_x")
```

## Over-fitting biopsy

It's a little data set but let's try and do a 80%/20% split into training and test sets.

```{r}
set.seed(42) # reproducible stochastic code
train_idx <- runif(nrow(biopsy_complete)) <= 0.8 # not exactly indices
biopsy_train <- filter(biopsy_complete, train_idx)
biopsy_test <- filter(biopsy_complete, !train_idx)
all_equal(biopsy_complete, bind_rows(biopsy_train, biopsy_test)) # sanity check

# Alternative with actual indices (mostly a matter of taste)
set.seed(42)
train_idx <- which(runif(nrow(biopsy_complete)) <= 0.8)
biopsy_train <- slice(biopsy_complete, train_idx)
biopsy_test <- slice(biopsy_complete, -train_idx)
all_equal(biopsy_complete, bind_rows(biopsy_train, biopsy_test)) # sanity check

# Normalise predictors and put them in matrix format
predictors_train <- biopsy_train %>% 
	select(-ID, -class) %>% 
	scale()

# Use normalisation factors from training predictors to scale the test predictors
predictors_test <- biopsy_test %>% 
	select(-ID, -class) %>% 
	scale(center = attr(predictors_train, "scaled:center"),
		  scale = attr(predictors_train, "scaled:scale"))

# Train model
lasso_biopsy_train <- glmnet(predictors_train, biopsy_train$class, family = "binomial")

# Prediction error in train and test sets
D_train <- predict(lasso_biopsy_train, predictors_train, type = "class") %>% 
	apply(2, function(.) mean(. != biopsy_train$class))

D_test <- predict(lasso_biopsy_train, predictors_test, type = "class") %>% 
	apply(2, function(.) mean(. != biopsy_test$class)) 

tibble(D_test = D_test[which.min(D_train)],
	   D_train = D_train[which.min(D_train)], 
	   D_diff_abs = scales::percent(D_test - D_train),
	   D_diff_rel = scales::percent((D_test - D_train) / D_train, big.mark = ","))

ggplot() + # could define the (common) x-axis variable already here, but simpler to do it for each geom
	coord_cartesian(ylim = c(0, NA)) +
	geom_line(aes(x = log(lasso_biopsy_train$lambda), y = D_train, colour = "Within-sample")) +
	geom_line(aes(x = log(lasso_biopsy_train$lambda), y = D_test, colour = "Hold-out")) +
	labs(y = "log(prediction error)", x = expression(log(lambda))) +
	scale_x_reverse() 
```

## Delassoing (NB! This is an active area of research, so don't rely too much one this)

The purpose of selective inference is to try and obtain reliable confidence intervals (based on correct p values) for associations that have already been selected using lasso regression. The `coef` function requires more arguments than normally because it needs to interpolate between the grid values of \$lambda\$ for which is was trained. Also note that you need to divide the `lambda` values with the number of observations (see `Examples` in `?fixedLassoInf`).

```{r}
plot(lasso_logreg, xvar = "lambda")
lambda <- exp(-2)
beta <- coef(lasso_logreg, x = biopsy_predictors, y = biopsy_complete$class, 
			 s = lambda/nrow(biopsy_complete), exact = TRUE) 
delasso_fit <- fixedLassoInf(biopsy_predictors, as.numeric(biopsy_complete$class) - 1, beta, 
							 lambda, "binomial", alpha = 0.05)
delasso_fit
```

## Cross-validation

Use cross-validation to find best $\lambda$ value. We found quite clear over-fitting above. Let's try to remedy this with cross-validation. We see that we need a fairly high penalty before we any real predictor selection, and that hurts the out-of-sample prediction performance.

```{r}
# Use the training set only to fit the CV model
lasso_logreg_cv <- cv.glmnet(predictors_train, biopsy_train$class, family = "binomial", nfolds = 10)
plot(lasso_logreg_cv) # note that the y axis does NOT start at 0, which can be misleading
with(lasso_logreg_cv, data.frame(lambda.min, lambda.1se))

# With ggplot2 (more control and prettier) -- there are two helper functions to tune the appearance of
# the labels showing the number of predictors (you can choose which to use in the geom_text line)
fade_text <- function(x, alpha = 0.2) {
	ifelse(paste(x) == lag(paste(x), default = ""), alpha, 1)
}
every_n <- function(x, n = 5) {
	seq_along(x) %% n == 0
}
with(lasso_logreg_cv, tibble(lambda, cvm, cvup, cvlo, nzero)) %>% 
	ggplot(aes(x = log(lambda))) +
		geom_vline(xintercept = log(lasso_logreg_cv$lambda.min), linetype = 2, size = 0.5, colour = "red") +
		geom_vline(xintercept = log(lasso_logreg_cv$lambda.1se), linetype = 2, size = 0.5, colour = "dodgerblue") +
		geom_linerange(aes(ymin = cvlo, ymax = cvup), size = 0.3) +
		geom_ribbon(aes(ymin = cvlo, ymax = cvup), alpha = 0.1) +
		geom_point(aes(y = cvm), shape = 18, size = 1.5) +
		geom_text(aes(y = max(cvup) * 1.05, label = nzero, alpha = fade_text(nzero)), size = 8 / ggplot2::.pt, show.legend = FALSE) +
		scale_alpha_identity() +
		coord_cartesian(ylim = c(0, NA)) + # force y axis start at 0 and end where the data do
		labs(x = expression(log(lambda)), y = lasso_logreg_cv$name) +
		theme_minimal()
```

## Evaluate performance of the CV model in the test set

```{r}
train_pred_min <- predict(lasso_logreg_cv, predictors_train, s = "lambda.min", type = "class")
train_pred_1se <- predict(lasso_logreg_cv, predictors_train, s = "lambda.1se", type = "class")

test_pred_min <- predict(lasso_logreg_cv, predictors_test, s = "lambda.min", type = "class")
test_pred_1se <- predict(lasso_logreg_cv, predictors_test, s = "lambda.1se", type = "class")

data.frame(train_min = mean(biopsy_train$class == train_pred_min),
		   test_min = mean(biopsy_test$class == test_pred_min),
		   train_1se = mean(biopsy_train$class == train_pred_1se),
		   test_1se = mean(biopsy_test$class == test_pred_1se))
```

## Cross-validation to pick best combination of $\lambda$ and $\alpha$

```{r}
alpha_lambda_cv_res <- expand.grid(lambda = exp(seq(-8, -1, length.out = 100)), # pretty much what's used in lasso_logreg_cv$lambda
								   alpha = seq(0, 1, 0.1)) %>% 
	dlply("alpha", function(d) cv.glmnet(predictors_train, biopsy_train$class, family = "binomial", nfolds = 10, lambda = d$lambda_seq)) %>% 
	llply(function(fit) mutate(tidy(fit), 
							   lambda_level = case_when(lambda == fit$lambda.min ~ "min", lambda == fit$lambda.1se ~ "1se"),
							   pred_err_mean = fit$cvm,
							   pred_err_up = fit$cvup,
							   pred_err_lo = fit$cvlo)) %>% 
	bind_rows(.id = "alpha") 

# Overlain plot of the best and "second best" (within 1 standard error of the best) predictions
ggplot(alpha_lambda_cv_res, aes(x = log(lambda), y = pred_err_mean, colour = alpha)) +
	coord_cartesian(xlim = c(-6.5, -3), ylim = c(0.15, 0.35)) +
	geom_line(alpha = 0.5) +
	geom_point(aes(shape = lambda_level), ~ filter(., !is.na(lambda_level)), size = 2) +
	labs(x = expression(log(lambda)), y = "Binomial deviance")

# And the best combination of alpha and lambda
alpha_lambda_cv_res %>% 
	slice(which.min(pred_err_mean))
```

```{r bootstrap, include=FALSE, eval=FALSE}
# Bootstrapping
data(PimaIndiansDiabetes2)
insulin <- PimaIndiansDiabetes2 %>% 
	filter(is.na(insulin))
qplot(insulin$insulin) # Very skewed, almost poisson-ish
mean(insulin) #
mean_jack <- jackknife(insulin, mean) # se_jack = 6.0

# Simple jackknife CI:
mean(insulin) + c(-1.96, 1.96) * mean_jack$jack.se

boot_mean <- bootstrap(insulin, 20000, mean)
hist(boot_mean$thetastar) 
  #' Shows that the central limit theorem has kicked in, as the distribution is
  #' essentially Gaussian, meaning that our estimates for the mean are stable.
# Simple bootstrap CI:
mean(boot_mean$thetastar) + c(-1.96, 1.96) * sd(boot_mean$thetastar)

# Bias-corrected CI:
bcanon(insulin, 2000, mean)$confpoints[c(1, 8), ]
```

```{r, eval=FALSE, echo=FALSE}
# this way nothing below is included when knitted
knitr::opts_chunk$set(inlude=FALSE, eval=FALSE, echo=FALSE)
```

# Exercise: cross-validation

```{r}
library(MASS)
data(biopsy)
biopsy_complete <- na.exclude(biopsy)
summary(biopsy_complete)

predictors <- biopsy_complete %>% 
	select(-ID, -class)
pca_fit <- prcomp(predictors, scale = TRUE)
df_pca <- data.frame(pca_fit$x[, 1:4], outcome = biopsy_complete$class)
glm_fit <- glm(outcome ~ PC1 + PC2 + PC3 + PC4, data = df_pca, family = binomial)
summary(glm_fit)
tidy(glm_fit)
```

## 1. LOO-CV error rate

```{r}
glm_fit_loocv <- cv.glm(df_pca, glm_fit)
glm_fit_loocv$delta
```

## 2. Use proper cost function

```{r}
glm_fit_loocv2 <- cv.glm(df_pca, glm_fit, cost = function(r, pi = 0) mean(abs(r-pi) > 0.5))
glm_fit_loocv2$delta
```

## 3. Difference between error rates, and their interpretation

We make a plot to talk about how a prediction yields different different costs depending on the cost function.

```{r}
expand.grid(y_obs = 0:1,
			y_pred = 0:100 / 100) %>% 
	mutate(cost_squared_error = (y_pred - y_obs)^2,
		   cost_binary = (abs(y_pred - y_obs) > 0.5) * 1,
		   cost_absolute_error = abs(y_pred - y_obs)) %>% 
	pivot_longer(starts_with("cost_"), names_to = "cost_fun", values_to = "cost") %>% 
	ggplot(aes(y_pred, cost, colour = cost_fun)) +
		geom_line() +
		facet_wrap(~ y_obs)
```

## 4. 10-fold CV

```{r}
# The error rates change quite a bit, which makes sense because 10-fold CV has a lot fewer folds than LOO which in this case has 683 folds. 
glm_fit_10cv <- update(glm_fit_loocv, K = 10)
glm_fit_10cv$delta

# glm_fit_10cv2 <- cv.glm(df_pca, glm_fit, cost = function(r, pi = 0) mean(abs(r-pi) > 0.5), K = 10)
glm_fit_10cv2 <- update(glm_fit_loocv2, K = 10)
glm_fit_10cv2$delta

ldply(list("LOO" = glm_fit_loocv, "LOO 2" = glm_fit_loocv2, "10-fold" = glm_fit_10cv, "10-fold 2" = glm_fit_10cv2),
	  with, delta) %>% 
	setNames(c("model", "error_rate", "corrected_error_rate"))
```

```{r}
# knitr::opts_chunk$set(include = FALSE)
```

# Exercise: penalised regression

## 1. + 2. Lasso regression

```{r}
load(url("https://www.biostatistics.dk/teaching/advtopicsA/data/lassodata.rda"))

# Remove colums with zero variation (because they all have the same values)
genotype <- genotype[, apply(genotype, 2, var) > 0]

# De-duplicate genotype matrix (= make sure all columns are unique, avoid perfect collinearity between columns)
genotype <- unique(genotype, MARGIN = 2)

# Normalise the matrix to put all predictors on similar scales
genotype <- scale(genotype) 

lasso <- glmnet(genotype, phenotype)
plot(lasso, xvar = "lambda")
```

## 3. Why does it normally make sense to normalise predictors

If predictors *not* on the same scale, those with large values will be discarded unduly from the model because they contribute a lot to the l1 norm.

## 4. Using CV to get reasonable estimate of $\lambda$

```{r}
lasso_cv <- cv.glmnet(genotype, phenotype, nfolds = 10)
plot(lasso_cv)
```

## 5. Obtain coefficients for "best" $\lambda$

```{r}
pretty_coefs(coef(lasso, s = lasso_cv$lambda.min)) 
```

## 6. Re-fit with correct family

```{r}
lasso_correct <- glmnet(genotype, phenotype, family = "binomial")
lasso_cv_correct <- cv.glmnet(genotype, phenotype, family = "binomial", nfolds = 10)
plot(lasso_cv_correct)
pretty_coefs(coef(lasso_correct, s = lasso_cv_correct$lambda.min))
```

Quite some difference between the sets of predictors kept:

```{r}
full_join(pretty_coefs(coef(lasso_correct, s = lasso_cv_correct$lambda.min)),
		  pretty_coefs(coef(lasso, s = lasso_cv$lambda.min)), by = "predictor") %>% 
	setNames(c("predictor", "coefficient_correct", "coefficient_incorrect"))
```

## 7. As ridge regression

```{r}
ridge_correct <- update(lasso_correct, alpha = 0)
plot(ridge_correct, xvar = "lambda")
ridge_cv_correct <- update(lasso_cv_correct, alpha = 0)

full_join(pretty_coefs(coef(lasso_correct, s = lasso_cv_correct$lambda.min)),
		  pretty_coefs(coef(ridge_correct, s = ridge_cv_correct$lambda.min)), by = "predictor") %>% 
	setNames(c("predictor", "coefficient_lasso", "coefficient_ridge")) %>% 
	mutate(ridge_order = rank(abs(coefficient_ridge)),
		   ridge_order = max(ridge_order) - ridge_order + 1)
```

## 8. Get idea about sparse solution using ridge results?

The closer to zero the coefficients are, they less important they are to the prediction (thanks to normalisation of predictors), so one could set a threshold and only keep predictors with abs(coefficient) above that.

## 9. Elastic net ($\alpha = 0.5$)

Ideally, one should do CV over $\alpha$ as well (see the example in the code from the lecture).

```{r}
elastic_correct <- update(lasso_correct, alpha = 0.5)
elastic_cv_correct <- update(lasso_cv_correct, alpha = 0.5)

full_join(pretty_coefs(coef(lasso_correct, s = lasso_cv_correct$lambda.min)),
		  pretty_coefs(coef(ridge_correct, s = ridge_cv_correct$lambda.min)), by = "predictor") %>% 
	full_join(pretty_coefs(coef(elastic_correct, s = elastic_cv_correct$lambda.min)), by = "predictor") %>% 
	setNames(c("predictor", "lasso", "ridge", "elastic"))
```

## 10. Delasso the results

```{r}
delasso_formula <- pretty_coefs(coef(lasso_correct, s = lasso_cv_correct$lambda.min)) %>% 
	filter(predictor != "(Intercept)") %>% 
	with(predictor) %>% 
	paste(collapse = "+") %>% 
	paste("outcome ~", .) %>% 
	as.formula()

delasso_formula
delasso_fit <- glm(delasso_formula, data = bind_cols(outcome = phenotype, data.frame(genotype)), family = binomial)
broom::tidy(delasso_fit)

confint(delasso_fit) # DON'T USE, THESE ARE NOT CORRECT BECAUSE WE'VE SELECTED PREDICTORS

mean(phenotype == (predict(delasso_fit, type = "response") > 0.5))
```

## 11. Selective inference (doesn't run)

Again, this is an active area of research so the package is not very stable and the results are probably not be relied upon for now. Included here to illustrate it can be done. Also, the `fixedLassoInf` function returns an error due to singularity (at least as of 3 November 2020) owing to some columns being so similar that they seem identical to the function (essentially, perfect collinearity).

```{r}
cosine_similarity <- function(df) {
	# df: matrix for whose columns the cosine similarity is to be computed
	
	as.matrix(df) %>% 
		{ t(.) %*% . / sqrt(colSums(.^2) %*% t(colSums(.^2))) } %>% 
		as.data.frame() %>% 
		rownames_to_column("target_row") %>% 
		pivot_longer(-target_row, names_to = "target_col", values_to = "cosine_similarity")
}

cosine_similarity(genotype) %>% 
	arrange(desc(cosine_similarity)) 
```

```{r, eval=FALSE}
lambda <- lasso_cv_correct$lambda.min
beta <- coef(lasso_correct, x = genotype, y = phenotype, exact = TRUE, s = lambda/nrow(genotype))
res <- fixedLassoInf(genotype, phenotype, beta, lambda, family = "binomial", alpha = 0.05, tol.beta = .Machine$double.eps)
```
